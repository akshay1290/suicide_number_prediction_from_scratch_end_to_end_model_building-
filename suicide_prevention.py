# -*- coding: utf-8 -*-
"""SUICIDE_PREVENTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vD38sw_Am2p9n-lf7AhxmKgj8RlqQ8vz
"""

# Import library for exploring dataset
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df1 = pd.read_csv("suicide(1).csv")
df2 = pd.read_csv("suicide(2).csv")

df1.head()

df2.head()

common_column = 'id'
df = pd.merge(df1, df2, on=common_column)
df.replace(['?', '/', '#'], np.nan, inplace=True)

df.head(10)

df.shape

df=df.drop('id',axis=1)

df.info()

df=df.drop('country-year',axis=1)

df.rename(columns={"suicides/100k pop":"suicides_pop","HDI for year":"HDI_for_year",
                  " gdp_for_year ($) ":"gdp_for_year"," gdp_per_capita ($) ":"gdp_per_capita",
                    "gdp_per_capita ($)":"gdp_per_capita"}, inplace=True)
print(df.columns)

df.isnull().sum()

df['age'].value_counts()

# Calculate the mode
mode_val = df['age'].mode()[0]

# Fill the null values
df['age'].fillna(mode_val, inplace=True)

df['year'].value_counts()

# Calculate the mode
median_val = df['year'].median()

# Fill the null values
df['year'].fillna(median_val, inplace=True)

df['gdp_per_capita'].value_counts()

# Calculate the mode
mean_val = df['gdp_per_capita'].mean()

# Fill the null values
df['gdp_per_capita'].fillna(mean_val, inplace=True)

df['population'].value_counts()

# Calculate the mode
median_val = df['population'].median()

# Fill the null values
df['population'].fillna(median_val, inplace=True)

df['HDI_for_year'].value_counts()

# Calculate the mode
median_val = df['HDI_for_year'].median()

# Fill the null values
df['HDI_for_year'].fillna(median_val, inplace=True)

df.describe()

df.groupby('age').mean()[['suicides_pop']].sort_values(by = 'suicides_pop', ascending = False)

age = df['age'].unique()
for a in age:
    print('Suicide Rate : {} old category'.format(a))
    print(df[df['age'] == a].groupby(['country']).sum()['suicides_pop'].sort_values(ascending = False)[:3])
    print()

df.max()

suicides_per_country = df.groupby('country').sum('suicides_no').reset_index().sort_values('suicides_no',ascending = False)
suicides_per_country = suicides_per_country[['country','suicides_no']]
suicides_per_country.head()

"""Observation:-

1. Most number of suicides occur in Russian Federation followed by United States
"""

suicides_per_country = df.groupby('country').sum('suicides_pop').reset_index().sort_values('suicides_pop',ascending = False)
suicides_per_country = suicides_per_country[['country','suicides_pop']]
suicides_per_country.head()

suicides_per_country = df.groupby('HDI_for_year').sum('suicides_pop').reset_index().sort_values('suicides_pop',ascending = False)
suicides_per_country = suicides_per_country[['HDI_for_year','suicides_pop']]
suicides_per_country.head()

suicides_per_gen = df.groupby('generation').sum('suicides_no').reset_index().sort_values('suicides_no',ascending = False)
suicides_per_gen = suicides_per_gen[['generation','suicides_no']]
suicides_per_gen.head()

"""Observation:-

Boomers have the highest suicide number, followed by silent generation and generation x and millenials
"""



df.info()

df['year'] = df['year'].astype('float64')
df['population'] = df['population'].astype('float64')

df_workingyear = df[df['year']<= 2011]
population_year = df_workingyear.groupby('year')['population'].sum().reset_index()
suicides_yearwise = df_workingyear.groupby('year')['suicides_no'].sum().reset_index()
suicides_yearwise

# defining numerical & categorical columns
numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']
categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O']

# print columns
print('We have {} numerical features : {}'.format(len(numeric_features), numeric_features))
print('\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))

fig = plt.figure(figsize=(15, 20))

for i in range(0, len(numeric_features)):
    ax = plt.subplot(10, 2, i+1)

    sns.scatterplot(data= df ,x='suicides_pop', y=numeric_features[i], color='b')
  
    plt.tight_layout()

df['easy_gdp_cal'] = round(df['gdp_per_capita']/(len(df['age'].unique() * 2)))

sns.scatterplot(data= df ,x='suicides_pop', y='easy_gdp_cal', color='b')

plt.figure(figsize=(12, 6));
Age_Country = pd.DataFrame(df.groupby(['age','sex'],sort=True)['suicides_pop'].sum()).reset_index()
plot1 = Age_Country.sort_values(by=['suicides_pop','age'], ascending=False)
plot1.reset_index()
g = sns.barplot(x='suicides_pop', y='age', data=Age_Country, hue = 'sex');
plt.xticks(rotation=90)

plt.title('Identify the Suicides committed by male/female different age groups');

suicide_ctry = pd.DataFrame(df.groupby(['country'])['suicides_pop'].mean()).reset_index()
plot2 = suicide_ctry.sort_values(by=['suicides_pop','country'], ascending=False).head(25)
plot2.reset_index()
g = sns.barplot(x='suicides_pop', y='country', data=plot2);
plt.xticks(rotation=90)
plt.title('Identify the Average number of Suicides committed wrt countries over the years');

#check the distribution of numerical features in dataset
for feature in numeric_features:
    data=df.copy()
    data[feature].hist(bins=25)
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.title(feature)
    plt.show()

#df['suicides_pop']=np.log(df.suicides_pop)

df.columns

df=df.drop('easy_gdp_cal',axis=1)

fig, ax = plt.subplots(figsize=(8, 6))
sns.set_style('whitegrid')
sns.countplot(x='sex',data=df,ax=ax)

# check for duplicates
df.duplicated().sum()

# Import label encoder 
from sklearn import preprocessing 
  
# label_encoder object knows how to understand word labels. 
label_encoder = preprocessing.LabelEncoder() 

df['generation']= label_encoder.fit_transform(df['generation'])
df['sex']= label_encoder.fit_transform(df['sex'])
df['age']= label_encoder.fit_transform(df['age'])
df['country']= label_encoder.fit_transform(df['country'])

df=df.drop('gdp_for_year',axis=1)

# Create boxplots for all numeric columns in the dataset
sns.set(style="whitegrid")
for col in df:
    sns.boxplot(data=df[col], orient="v", palette="Set2")
    plt.title(col)
    plt.show()

# Identify the columns with potential outliers
outlier_cols = ['HDI_for_year','gdp_per_capita','suicides_pop', 'population']

# Replace outliers with the upper and lower bounds
for col in outlier_cols:
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    upper_bound = q3 + 1.5*iqr
    lower_bound = q1 - 1.5*iqr
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])

# Create boxplots for all numeric columns in the dataset
sns.set(style="whitegrid")
for col in df:
    sns.boxplot(data=df[col], orient="v", palette="Set2")
    plt.title(col)
    plt.show()

df.corr()

import numpy as np

class DecisionTreeRegressor:
    def __init__(self, max_depth=None, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None

    def mean_squared_error(self, y):
        return np.mean((y - np.mean(y)) ** 2)

    def split_data(self, X, y, feature_idx, threshold):
        left_mask = X[:, feature_idx] <= threshold
        right_mask = X[:, feature_idx] > threshold
        X_left, y_left = X[left_mask], y[left_mask]
        X_right, y_right = X[right_mask], y[right_mask]
        return X_left, y_left, X_right, y_right

    def find_best_split(self, X, y):
        best_feature_idx, best_threshold, best_mse = None, None, np.inf
        for feature_idx in range(X.shape[1]):
            for threshold in np.unique(X[:, feature_idx]):
                X_left, y_left, X_right, y_right = self.split_data(X, y, feature_idx, threshold)
                if len(y_left) < self.min_samples_split or len(y_right) < self.min_samples_split:
                    continue
                mse_left, mse_right = self.mean_squared_error(y_left), self.mean_squared_error(y_right)
                mse = mse_left + mse_right
                if mse < best_mse:
                    best_feature_idx, best_threshold, best_mse = feature_idx, threshold, mse
        return best_feature_idx, best_threshold, best_mse

    def build_tree(self, X, y, depth):
        if depth == self.max_depth or len(y) < self.min_samples_split:
            return np.mean(y)
        feature_idx, threshold, mse = self.find_best_split(X, y)
        if mse == np.inf:
            return np.mean(y)
        X_left, y_left, X_right, y_right = self.split_data(X, y, feature_idx, threshold)
        left_node = self.build_tree(X_left, y_left, depth + 1)
        right_node = self.build_tree(X_right, y_right, depth + 1)
        return {"feature_idx": feature_idx, "threshold": threshold, "left_node": left_node, "right_node": right_node}

    def fit(self, X, y):
        self.tree = self.build_tree(X, y, 0)
    def set_params(self, **params):
      '''function is used to set the values of the attributes of a decision tree object. The function takes a variable 
      number of keyword arguments (**params), 
      which are pairs of attribute names and their corresponding values that should be set for the decision tree object'''
      for param, value in params.items():
        setattr(self, param, value)
        return self

    def predict(self, X):
        def predict_row(row, node):
            if isinstance(node, float):
                return node
            if row[node["feature_idx"]] <= node["threshold"]:
                return predict_row(row, node["left_node"])
            else:
                return predict_row(row, node["right_node"])
        return np.array([predict_row(row, self.tree) for row in X])
    def mean_squared_errorr(self,y_true, y_pred):
   
      # Check if the lengths of both arrays are equal
      if len(y_true) != len(y_pred):
          raise ValueError("Length of y_true and y_pred should be the same.")
      
      # Calculate the squared differences between the true and predicted values
      squared_differences = [(y_true[i] - y_pred[i])**2 for i in range(len(y_true))]
      
      # Calculate the mean of the squared differences
      mse = sum(squared_differences) / len(squared_differences)
      
      return mse

plt.figure(figsize = (15,10))
sns.heatmap(df.corr(), cmap="CMRmap", annot=True)
plt.show()

df.head()

df=df.drop(['generation','HDI_for_year','country'],axis=1)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

df[['gdp_per_capita', 'suicides_no', 'population', 'suicides_pop']] = scaler.fit_transform(df[['gdp_per_capita', 'suicides_no', 'population', 'suicides_pop']])

#defining dependent and independent variable as y and x
X = df.drop('suicides_no',axis=1).values
y = df['suicides_no'].values

X.reshape(1, -1)
y.reshape(1, -1)

from sklearn.model_selection import train_test_split



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

regressor = DecisionTreeRegressor(min_samples_split=4, max_depth=4)
regressor.fit(X_train,y_train)

y_pred = regressor.predict(X_test)

y_pred

import numpy as np
def mean_squared_error(y_true, y_pred):
    """
    Calculates the mean squared error between y_true and y_pred.
    :param y_true: A list or array of true values
    :param y_pred: A list or array of predicted values
    :return: The mean squared error between y_true and y_pred
    """
    # Get the length of the arrays
    n = len(y_true)
    
    # Calculate the sum of squared differences
    sum_squared_diff = sum((y_true[i] - y_pred[i])**2 for i in range(n))
    
    # Calculate the mean squared error
    mse = sum_squared_diff / n
    
    return mse

mse = mean_squared_error(y_test, y_pred)
mse



### Create a Pickle file using serialization 
import pickle
pickle_out = open("regressor.pkl","wb")
pickle.dump(regressor, pickle_out)
pickle_out.close()


df.head()

regressor.predict([[800,1987,1,5,21,312900]])